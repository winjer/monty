use std::collections::hash_map::DefaultHasher;
use std::collections::HashSet;
use std::fmt::Write;
use std::hash::{Hash, Hasher};

use ahash::AHashSet;

use crate::args::ArgValues;
use crate::exception::ExcType;
use crate::intern::{FunctionId, Interns};
use crate::resource::{ResourceError, ResourceTracker};
use crate::run_frame::RunResult;
use crate::types::{Bytes, Dict, FrozenSet, List, PyTrait, Set, Str, Tuple, Type};
use crate::value::{Attr, Value};

/// Unique identifier for values stored inside the heap arena.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub struct HeapId(usize);

impl HeapId {
    /// Returns the raw index value.
    #[inline]
    pub fn index(self) -> usize {
        self.0
    }
}

/// HeapData captures every runtime value that must live in the arena.
///
/// Each variant wraps a type that implements `AbstractValue`, providing
/// Python-compatible operations. The trait is manually implemented to dispatch
/// to the appropriate variant's implementation.
///
/// Note: The `Value` variant is special - it wraps boxed immediate values
/// that need heap identity (e.g., when `id()` is called on an int).
#[derive(Debug)]
pub enum HeapData {
    Str(Str),
    Bytes(Bytes),
    List(List),
    Tuple(Tuple),
    Dict(Dict),
    Set(Set),
    FrozenSet(FrozenSet),
    /// A closure: a function that captures variables from enclosing scopes.
    ///
    /// Contains a reference to the function definition, a vector of captured cell HeapIds,
    /// and evaluated default values (if any). When the closure is called, these cells are
    /// passed to the RunFrame for variable access. When the closure is dropped, we must
    /// decrement the ref count on each captured cell and each default value.
    Closure(FunctionId, Vec<HeapId>, Vec<Value>),
    /// A function with evaluated default parameter values (non-closure).
    ///
    /// Contains a reference to the function definition and the evaluated default values.
    /// When the function is called, defaults are cloned for missing optional parameters.
    /// When dropped, we must decrement the ref count on each default value.
    FunctionDefaults(FunctionId, Vec<Value>),
    /// A cell wrapping a single mutable value for closure support.
    ///
    /// Cells enable nonlocal variable access by providing a heap-allocated
    /// container that can be shared between a function and its nested functions.
    /// Both the outer function and inner function hold references to the same
    /// cell, allowing modifications to propagate across scope boundaries.
    Cell(Value),
}

impl HeapData {
    /// Computes hash for immutable heap types that can be used as dict keys.
    ///
    /// Returns Some(hash) for immutable types (Str, Bytes, Tuple of hashables).
    /// Returns None for mutable types (List, Dict) which cannot be dict keys.
    ///
    /// This is called lazily when the value is first used as a dict key,
    /// avoiding unnecessary hash computation for values that are never used as keys.
    fn compute_hash_if_immutable(&self, heap: &mut Heap<impl ResourceTracker>, interns: &Interns) -> Option<u64> {
        match self {
            Self::Str(s) => {
                let mut hasher = DefaultHasher::new();
                s.as_str().hash(&mut hasher);
                Some(hasher.finish())
            }
            Self::Bytes(b) => {
                let mut hasher = DefaultHasher::new();
                b.as_slice().hash(&mut hasher);
                Some(hasher.finish())
            }
            Self::Tuple(t) => {
                // Tuple is hashable only if all elements are hashable
                let mut hasher = DefaultHasher::new();
                for obj in t.as_vec() {
                    match obj.py_hash(heap, interns) {
                        Some(h) => h.hash(&mut hasher),
                        None => return None, // Contains unhashable element
                    }
                }
                Some(hasher.finish())
            }
            Self::FrozenSet(fs) => {
                // FrozenSet hash is XOR of element hashes (order-independent)
                fs.compute_hash(heap, interns)
            }
            Self::Closure(f, _, _) | Self::FunctionDefaults(f, _) => {
                let mut hasher = DefaultHasher::new();
                // TODO, this is NOT proper hashing, we should somehow hash the function properly
                f.hash(&mut hasher);
                Some(hasher.finish())
            }
            // Mutable types cannot be hashed (Cell is handled specially in get_or_compute_hash)
            Self::List(_) | Self::Dict(_) | Self::Set(_) | Self::Cell(_) => None,
        }
    }
}

/// Manual implementation of AbstractValue dispatch for HeapData.
///
/// This provides efficient dispatch without boxing overhead by matching on
/// the enum variant and delegating to the inner type's implementation.
impl PyTrait for HeapData {
    fn py_type(&self, heap: Option<&Heap<impl ResourceTracker>>) -> Type {
        match self {
            Self::Str(s) => s.py_type(heap),
            Self::Bytes(b) => b.py_type(heap),
            Self::List(l) => l.py_type(heap),
            Self::Tuple(t) => t.py_type(heap),
            Self::Dict(d) => d.py_type(heap),
            Self::Set(s) => s.py_type(heap),
            Self::FrozenSet(fs) => fs.py_type(heap),
            Self::Closure(_, _, _) | Self::FunctionDefaults(_, _) => Type::Function,
            Self::Cell(_) => Type::Cell,
        }
    }

    fn py_estimate_size(&self) -> usize {
        match self {
            Self::Str(s) => s.py_estimate_size(),
            Self::Bytes(b) => b.py_estimate_size(),
            Self::List(l) => l.py_estimate_size(),
            Self::Tuple(t) => t.py_estimate_size(),
            Self::Dict(d) => d.py_estimate_size(),
            Self::Set(s) => s.py_estimate_size(),
            Self::FrozenSet(fs) => fs.py_estimate_size(),
            // TODO: should include size of captured cells and defaults
            Self::Closure(_, _, _) | Self::FunctionDefaults(_, _) => 0,
            Self::Cell(v) => std::mem::size_of::<Value>() + v.py_estimate_size(),
        }
    }

    fn py_len(&self, heap: &Heap<impl ResourceTracker>, interns: &Interns) -> Option<usize> {
        match self {
            Self::Str(s) => PyTrait::py_len(s, heap, interns),
            Self::Bytes(b) => PyTrait::py_len(b, heap, interns),
            Self::List(l) => PyTrait::py_len(l, heap, interns),
            Self::Tuple(t) => PyTrait::py_len(t, heap, interns),
            Self::Dict(d) => PyTrait::py_len(d, heap, interns),
            Self::Set(s) => PyTrait::py_len(s, heap, interns),
            Self::FrozenSet(fs) => PyTrait::py_len(fs, heap, interns),
            _ => None, // Cells don't have length
        }
    }

    fn py_eq(&self, other: &Self, heap: &mut Heap<impl ResourceTracker>, interns: &Interns) -> bool {
        match (self, other) {
            (Self::Str(a), Self::Str(b)) => a.py_eq(b, heap, interns),
            (Self::Bytes(a), Self::Bytes(b)) => a.py_eq(b, heap, interns),
            (Self::List(a), Self::List(b)) => a.py_eq(b, heap, interns),
            (Self::Tuple(a), Self::Tuple(b)) => a.py_eq(b, heap, interns),
            (Self::Dict(a), Self::Dict(b)) => a.py_eq(b, heap, interns),
            (Self::Set(a), Self::Set(b)) => a.py_eq(b, heap, interns),
            (Self::FrozenSet(a), Self::FrozenSet(b)) => a.py_eq(b, heap, interns),
            (Self::Closure(a_id, a_cells, _), Self::Closure(b_id, b_cells, _)) => *a_id == *b_id && a_cells == b_cells,
            (Self::FunctionDefaults(a_id, _), Self::FunctionDefaults(b_id, _)) => *a_id == *b_id,
            // Cells compare by identity only (handled at Value level via HeapId comparison)
            (Self::Cell(_), Self::Cell(_)) => false,
            _ => false, // Different types are never equal
        }
    }

    fn py_dec_ref_ids(&mut self, stack: &mut Vec<HeapId>) {
        match self {
            Self::Str(s) => s.py_dec_ref_ids(stack),
            Self::Bytes(b) => b.py_dec_ref_ids(stack),
            Self::List(l) => l.py_dec_ref_ids(stack),
            Self::Tuple(t) => t.py_dec_ref_ids(stack),
            Self::Dict(d) => d.py_dec_ref_ids(stack),
            Self::Set(s) => s.py_dec_ref_ids(stack),
            Self::FrozenSet(fs) => fs.py_dec_ref_ids(stack),
            Self::Closure(_, cells, defaults) => {
                // Decrement ref count for captured cells
                stack.extend(cells.iter().copied());
                // Decrement ref count for default values that are heap references
                for default in defaults.iter_mut() {
                    default.py_dec_ref_ids(stack);
                }
            }
            Self::FunctionDefaults(_, defaults) => {
                // Decrement ref count for default values that are heap references
                for default in defaults.iter_mut() {
                    default.py_dec_ref_ids(stack);
                }
            }
            Self::Cell(v) => v.py_dec_ref_ids(stack),
        }
    }

    fn py_bool(&self, heap: &Heap<impl ResourceTracker>, interns: &Interns) -> bool {
        match self {
            Self::Str(s) => s.py_bool(heap, interns),
            Self::Bytes(b) => b.py_bool(heap, interns),
            Self::List(l) => l.py_bool(heap, interns),
            Self::Tuple(t) => t.py_bool(heap, interns),
            Self::Dict(d) => d.py_bool(heap, interns),
            Self::Set(s) => s.py_bool(heap, interns),
            Self::FrozenSet(fs) => fs.py_bool(heap, interns),
            Self::Closure(_, _, _) | Self::FunctionDefaults(_, _) => true,
            Self::Cell(_) => true, // Cells are always truthy
        }
    }

    fn py_repr_fmt(
        &self,
        f: &mut impl Write,
        heap: &Heap<impl ResourceTracker>,
        heap_ids: &mut AHashSet<HeapId>,
        interns: &Interns,
    ) -> std::fmt::Result {
        match self {
            Self::Str(s) => s.py_repr_fmt(f, heap, heap_ids, interns),
            Self::Bytes(b) => b.py_repr_fmt(f, heap, heap_ids, interns),
            Self::List(l) => l.py_repr_fmt(f, heap, heap_ids, interns),
            Self::Tuple(t) => t.py_repr_fmt(f, heap, heap_ids, interns),
            Self::Dict(d) => d.py_repr_fmt(f, heap, heap_ids, interns),
            Self::Set(s) => s.py_repr_fmt(f, heap, heap_ids, interns),
            Self::FrozenSet(fs) => fs.py_repr_fmt(f, heap, heap_ids, interns),
            Self::Closure(f_id, _, _) | Self::FunctionDefaults(f_id, _) => {
                interns.get_function(*f_id).py_repr_fmt(f, interns, 0)
            }
            // Cell repr shows the contained value's type
            Self::Cell(v) => write!(f, "<cell: {} object>", v.py_type(Some(heap))),
        }
    }
    // py_str is always the same as py_repr which is the default impl

    fn py_add(
        &self,
        other: &Self,
        heap: &mut Heap<impl ResourceTracker>,
        interns: &Interns,
    ) -> Result<Option<Value>, crate::resource::ResourceError> {
        match (self, other) {
            (Self::Str(a), Self::Str(b)) => a.py_add(b, heap, interns),
            (Self::Bytes(a), Self::Bytes(b)) => a.py_add(b, heap, interns),
            (Self::List(a), Self::List(b)) => a.py_add(b, heap, interns),
            (Self::Tuple(a), Self::Tuple(b)) => a.py_add(b, heap, interns),
            (Self::Dict(a), Self::Dict(b)) => a.py_add(b, heap, interns),
            // Cells don't support arithmetic operations
            _ => Ok(None),
        }
    }

    fn py_sub(
        &self,
        other: &Self,
        heap: &mut Heap<impl ResourceTracker>,
    ) -> Result<Option<Value>, crate::resource::ResourceError> {
        match (self, other) {
            (Self::Str(a), Self::Str(b)) => a.py_sub(b, heap),
            (Self::Bytes(a), Self::Bytes(b)) => a.py_sub(b, heap),
            (Self::List(a), Self::List(b)) => a.py_sub(b, heap),
            (Self::Tuple(a), Self::Tuple(b)) => a.py_sub(b, heap),
            (Self::Dict(a), Self::Dict(b)) => a.py_sub(b, heap),
            (Self::Set(a), Self::Set(b)) => a.py_sub(b, heap),
            (Self::FrozenSet(a), Self::FrozenSet(b)) => a.py_sub(b, heap),
            // Cells don't support arithmetic operations
            _ => Ok(None),
        }
    }

    fn py_mod(&self, other: &Self) -> Option<Value> {
        match (self, other) {
            (Self::Str(a), Self::Str(b)) => a.py_mod(b),
            (Self::Bytes(a), Self::Bytes(b)) => a.py_mod(b),
            (Self::List(a), Self::List(b)) => a.py_mod(b),
            (Self::Tuple(a), Self::Tuple(b)) => a.py_mod(b),
            (Self::Dict(a), Self::Dict(b)) => a.py_mod(b),
            // Cells don't support arithmetic operations
            _ => None,
        }
    }

    fn py_mod_eq(&self, other: &Self, right_value: i64) -> Option<bool> {
        match (self, other) {
            (Self::Str(a), Self::Str(b)) => a.py_mod_eq(b, right_value),
            (Self::Bytes(a), Self::Bytes(b)) => a.py_mod_eq(b, right_value),
            (Self::List(a), Self::List(b)) => a.py_mod_eq(b, right_value),
            (Self::Tuple(a), Self::Tuple(b)) => a.py_mod_eq(b, right_value),
            (Self::Dict(a), Self::Dict(b)) => a.py_mod_eq(b, right_value),
            // Cells don't support arithmetic operations
            _ => None,
        }
    }

    fn py_iadd(
        &mut self,
        other: Value,
        heap: &mut Heap<impl ResourceTracker>,
        self_id: Option<HeapId>,
        interns: &Interns,
    ) -> Result<bool, crate::resource::ResourceError> {
        match self {
            Self::Str(s) => s.py_iadd(other, heap, self_id, interns),
            Self::Bytes(b) => b.py_iadd(other, heap, self_id, interns),
            Self::List(l) => l.py_iadd(other, heap, self_id, interns),
            Self::Tuple(t) => t.py_iadd(other, heap, self_id, interns),
            Self::Dict(d) => d.py_iadd(other, heap, self_id, interns),
            _ => Ok(false),
        }
    }

    fn py_call_attr(
        &mut self,
        heap: &mut Heap<impl ResourceTracker>,
        attr: &Attr,
        args: ArgValues,
        interns: &Interns,
    ) -> RunResult<Value> {
        match self {
            Self::Str(s) => s.py_call_attr(heap, attr, args, interns),
            Self::Bytes(b) => b.py_call_attr(heap, attr, args, interns),
            Self::List(l) => l.py_call_attr(heap, attr, args, interns),
            Self::Tuple(t) => t.py_call_attr(heap, attr, args, interns),
            Self::Dict(d) => d.py_call_attr(heap, attr, args, interns),
            Self::Set(s) => s.py_call_attr(heap, attr, args, interns),
            Self::FrozenSet(fs) => fs.py_call_attr(heap, attr, args, interns),
            _ => Err(ExcType::attribute_error(self.py_type(Some(heap)), attr)),
        }
    }

    fn py_getitem(&self, key: &Value, heap: &mut Heap<impl ResourceTracker>, interns: &Interns) -> RunResult<Value> {
        match self {
            Self::Str(s) => s.py_getitem(key, heap, interns),
            Self::Bytes(b) => b.py_getitem(key, heap, interns),
            Self::List(l) => l.py_getitem(key, heap, interns),
            Self::Tuple(t) => t.py_getitem(key, heap, interns),
            Self::Dict(d) => d.py_getitem(key, heap, interns),
            _ => Err(ExcType::type_error_not_sub(self.py_type(Some(heap)))),
        }
    }

    fn py_setitem(
        &mut self,
        key: Value,
        value: Value,
        heap: &mut Heap<impl ResourceTracker>,
        interns: &Interns,
    ) -> RunResult<()> {
        match self {
            Self::Str(s) => s.py_setitem(key, value, heap, interns),
            Self::Bytes(b) => b.py_setitem(key, value, heap, interns),
            Self::List(l) => l.py_setitem(key, value, heap, interns),
            Self::Tuple(t) => t.py_setitem(key, value, heap, interns),
            Self::Dict(d) => d.py_setitem(key, value, heap, interns),
            _ => Err(ExcType::type_error_not_sub_assignment(self.py_type(Some(heap)))),
        }
    }
}

/// Hash caching state stored alongside each heap entry.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
enum HashState {
    /// Hash has not yet been computed but the value might be hashable.
    Unknown,
    /// Cached hash value for immutable types that have been hashed at least once.
    Cached(u64),
    /// Value is unhashable (mutable types or tuples containing unhashables).
    Unhashable,
}

impl HashState {
    fn for_data(data: &HeapData) -> Self {
        match data {
            // Cells are hashable by identity (like all Python objects without __hash__ override)
            // FrozenSet is immutable and hashable
            HeapData::Str(_)
            | HeapData::Bytes(_)
            | HeapData::Tuple(_)
            | HeapData::FrozenSet(_)
            | HeapData::Cell(_)
            | HeapData::Closure(_, _, _)
            | HeapData::FunctionDefaults(_, _) => Self::Unknown,
            // Mutable containers are unhashable
            HeapData::List(_) | HeapData::Dict(_) | HeapData::Set(_) => Self::Unhashable,
        }
    }
}

/// A single entry inside the heap arena, storing refcount, payload, and hash metadata.
///
/// The `hash_state` field tracks whether the heap entry is hashable and, if so,
/// caches the computed hash. Mutable types (List, Dict) start as `Unhashable` and
/// will raise TypeError if used as dict keys.
///
/// The `data` field is an Option to support temporary borrowing: when methods like
/// `with_entry_mut` or `call_attr` need mutable access to both the data and the heap,
/// they can `.take()` the data out (leaving `None`), pass `&mut Heap` to user code,
/// then restore the data. This avoids unsafe code while keeping `refcount` accessible
/// for `inc_ref`/`dec_ref` during the borrow.
#[derive(Debug)]
struct HeapValue {
    refcount: usize,
    /// The payload data. Temporarily `None` while borrowed via `with_entry_mut`/`call_attr`.
    data: Option<HeapData>,
    /// Current hashing status / cached hash value
    hash_state: HashState,
}

/// Reference-counted arena that backs all heap-only runtime values.
///
/// Uses a free list to reuse slots from freed values, keeping memory usage
/// constant for long-running loops that repeatedly allocate and free values.
/// When an value is freed via `dec_ref`, its slot ID is added to the free list.
/// New allocations pop from the free list when available, otherwise append.
///
/// Generic over `T: ResourceTracker` to support different resource tracking strategies.
/// When `T = NoLimitTracker` (the default), all resource checks compile away to no-ops.
#[derive(Debug)]
pub struct Heap<T: ResourceTracker> {
    entries: Vec<Option<HeapValue>>,
    /// IDs of freed slots available for reuse. Populated by `dec_ref`, consumed by `allocate`.
    free_list: Vec<HeapId>,
    /// Resource tracker for enforcing limits and scheduling GC.
    tracker: T,
}

macro_rules! take_data {
    ($self:ident, $id:expr, $func_name:literal) => {
        $self
            .entries
            .get_mut($id.index())
            .expect(concat!("Heap::", $func_name, ": slot missing"))
            .as_mut()
            .expect(concat!("Heap::", $func_name, ": object already freed"))
            .data
            .take()
            .expect(concat!("Heap::", $func_name, ": data already borrowed"))
    };
}

macro_rules! restore_data {
    ($self:ident, $id:expr, $new_data:expr, $func_name:literal) => {{
        let entry = $self
            .entries
            .get_mut($id.index())
            .expect(concat!("Heap::", $func_name, ": slot missing"))
            .as_mut()
            .expect(concat!("Heap::", $func_name, ": object already freed"));
        entry.data = Some($new_data);
    }};
}

impl<T: ResourceTracker> Heap<T> {
    /// Creates a new heap with the given resource tracker.
    ///
    /// Use this to create heaps with custom resource limits or GC scheduling.
    pub fn new(capacity: usize, tracker: T) -> Self {
        Self {
            entries: Vec::with_capacity(capacity),
            free_list: Vec::new(),
            tracker,
        }
    }

    /// Returns a reference to the resource tracker.
    pub fn tracker(&self) -> &T {
        &self.tracker
    }

    /// Returns a mutable reference to the resource tracker.
    pub fn tracker_mut(&mut self) -> &mut T {
        &mut self.tracker
    }

    /// Allocates a new heap entry.
    ///
    /// Returns `Err(ResourceError)` if allocation would exceed configured limits.
    /// Use this when you need to handle resource limit errors gracefully.
    pub fn allocate(&mut self, data: HeapData) -> Result<HeapId, ResourceError> {
        self.tracker.on_allocate(|| data.py_estimate_size())?;

        let hash_state = HashState::for_data(&data);
        let new_entry = HeapValue {
            refcount: 1,
            data: Some(data),
            hash_state,
        };

        let id = if let Some(id) = self.free_list.pop() {
            // Reuse a freed slot
            self.entries[id.index()] = Some(new_entry);
            id
        } else {
            // No free slots, append new entry
            let id = self.entries.len();
            self.entries.push(Some(new_entry));
            HeapId(id)
        };

        Ok(id)
    }

    /// Allocates a new cell containing the given value.
    ///
    /// Cells are used for closure support, allowing values to be shared between
    /// a function and its nested closures. The cell is created with refcount 1.
    ///
    /// Note: The contained value's refcount is NOT incremented. The caller is
    /// responsible for ensuring proper reference counting of the value before
    /// putting it in a cell (typically by cloning with `clone_with_heap`).
    pub fn alloc_cell(&mut self, value: Value) -> HeapId {
        // Cell allocation is considered infallible - cells are small and essential
        // for closure support. Panic if allocation fails (should be rare).
        self.allocate(HeapData::Cell(value))
            .expect("cell allocation failed - out of resources")
    }

    /// Increments the reference count for an existing heap entry.
    ///
    /// # Panics
    /// Panics if the value ID is invalid or the value has already been freed.
    pub fn inc_ref(&mut self, id: HeapId) {
        let value = self
            .entries
            .get_mut(id.index())
            .expect("Heap::inc_ref: slot missing")
            .as_mut()
            .expect("Heap::inc_ref: object already freed");
        value.refcount += 1;
    }

    /// Decrements the reference count and frees the value (plus children) once it hits zero.
    ///
    /// When an value is freed, its slot ID is added to the free list for reuse by
    /// future allocations. Uses recursion for child cleanup - avoiding repeated Vec
    /// allocations and benefiting from call stack locality.
    ///
    /// # Panics
    /// Panics if the value ID is invalid or the value has already been freed.
    pub fn dec_ref(&mut self, id: HeapId) {
        let slot = self.entries.get_mut(id.index()).expect("Heap::dec_ref: slot missing");
        let entry = slot.as_mut().expect("Heap::dec_ref: object already freed");
        if entry.refcount > 1 {
            entry.refcount -= 1;
        } else if let Some(value) = slot.take() {
            // refcount == 1, free the value and add slot to free list for reuse
            self.free_list.push(id);

            // Notify tracker of freed memory
            if let Some(ref data) = value.data {
                self.tracker.on_free(|| data.py_estimate_size());
            }

            // Collect child IDs and mark Values as Dereferenced (when dec-ref-check enabled)
            if let Some(mut data) = value.data {
                let mut child_ids = Vec::new();
                data.py_dec_ref_ids(&mut child_ids);
                drop(data);
                // Recursively decrement children
                for child_id in child_ids {
                    self.dec_ref(child_id);
                }
            }
        }
    }

    /// Returns an immutable reference to the heap data stored at the given ID.
    ///
    /// # Panics
    /// Panics if the value ID is invalid, the value has already been freed,
    /// or the data is currently borrowed via `with_entry_mut`/`call_attr`.
    #[must_use]
    pub fn get(&self, id: HeapId) -> &HeapData {
        self.entries
            .get(id.index())
            .expect("Heap::get: slot missing")
            .as_ref()
            .expect("Heap::get: object already freed")
            .data
            .as_ref()
            .expect("Heap::get: data currently borrowed")
    }

    /// Returns a mutable reference to the heap data stored at the given ID.
    ///
    /// # Panics
    /// Panics if the value ID is invalid, the value has already been freed,
    /// or the data is currently borrowed via `with_entry_mut`/`call_attr`.
    pub fn get_mut(&mut self, id: HeapId) -> &mut HeapData {
        self.entries
            .get_mut(id.index())
            .expect("Heap::get_mut: slot missing")
            .as_mut()
            .expect("Heap::get_mut: object already freed")
            .data
            .as_mut()
            .expect("Heap::get_mut: data currently borrowed")
    }

    /// Returns or computes the hash for the heap entry at the given ID.
    ///
    /// Hashes are computed lazily on first use and then cached. Returns
    /// Some(hash) for immutable types (Str, Bytes, hashable Tuple), None
    /// for mutable types (List, Dict).
    ///
    /// # Panics
    /// Panics if the value ID is invalid or the value has already been freed.
    pub fn get_or_compute_hash(&mut self, id: HeapId, interns: &Interns) -> Option<u64> {
        let entry = self
            .entries
            .get_mut(id.index())
            .expect("Heap::get_or_compute_hash: slot missing")
            .as_mut()
            .expect("Heap::get_or_compute_hash: object already freed");

        match entry.hash_state {
            HashState::Unhashable => return None,
            HashState::Cached(hash) => return Some(hash),
            HashState::Unknown => {}
        }

        // Handle Cell specially - uses identity-based hashing (like Python cell objects)
        if let Some(HeapData::Cell(_)) = &entry.data {
            let mut hasher = DefaultHasher::new();
            id.hash(&mut hasher);
            let hash = hasher.finish();
            entry.hash_state = HashState::Cached(hash);
            return Some(hash);
        }

        // Compute hash lazily - need to temporarily take data to avoid borrow conflict
        let data = entry.data.take().expect("Heap::get_or_compute_hash: data borrowed");
        let hash = data.compute_hash_if_immutable(self, interns);

        // Restore data and cache the hash if computed
        let entry = self
            .entries
            .get_mut(id.index())
            .expect("Heap::get_or_compute_hash: slot missing after compute")
            .as_mut()
            .expect("Heap::get_or_compute_hash: object freed during compute");
        entry.data = Some(data);
        entry.hash_state = match hash {
            Some(value) => HashState::Cached(value),
            None => HashState::Unhashable,
        };
        hash
    }

    /// Calls an attribute on the heap entry at `id` while temporarily taking ownership
    /// of its payload so we can borrow the heap again inside the call. This avoids the
    /// borrow checker conflict that arises when attribute implementations also need
    /// mutable access to the heap (e.g. for refcounting).
    pub fn call_attr(&mut self, id: HeapId, attr: &Attr, args: ArgValues, interns: &Interns) -> RunResult<Value> {
        // Take data out in a block so the borrow of self.entries ends
        let mut data = take_data!(self, id, "call_attr");

        let result = data.py_call_attr(self, attr, args, interns);

        // Restore data
        let entry = self
            .entries
            .get_mut(id.index())
            .expect("Heap::call_attr: slot missing")
            .as_mut()
            .expect("Heap::call_attr: object already freed");
        entry.data = Some(data);
        result
    }

    /// Gives mutable access to a heap entry while allowing reentrant heap usage
    /// inside the closure (e.g. to read other values or allocate results).
    ///
    /// The data is temporarily taken from the heap entry, so the closure can safely
    /// mutate both the entry data and the heap (e.g. to allocate new values).
    /// The data is automatically restored after the closure completes.
    pub fn with_entry_mut<F, R>(&mut self, id: HeapId, f: F) -> R
    where
        F: FnOnce(&mut Heap<T>, &mut HeapData) -> R,
    {
        // Take data out in a block so the borrow of self.entries ends
        let mut data = take_data!(self, id, "with_entry_mut");

        let result = f(self, &mut data);

        // Restore data
        restore_data!(self, id, data, "with_entry_mut");
        result
    }

    /// Temporarily takes ownership of two heap entries so their data can be borrowed
    /// simultaneously while still permitting mutable access to the heap (e.g. to
    /// allocate results). Automatically restores both entries after the closure
    /// finishes executing.
    pub fn with_two<F, R>(&mut self, left: HeapId, right: HeapId, f: F) -> R
    where
        F: FnOnce(&mut Heap<T>, &HeapData, &HeapData) -> R,
    {
        if left == right {
            // Same value - take data once and pass it twice
            let data = take_data!(self, left, "with_two");

            let result = f(self, &data, &data);

            restore_data!(self, left, data, "with_two");
            result
        } else {
            // Different values - take both
            let left_data = take_data!(self, left, "with_two (left)");
            let right_data = take_data!(self, right, "with_two (right)");

            let result = f(self, &left_data, &right_data);

            // Restore in reverse order
            restore_data!(self, right, right_data, "with_two (right)");
            restore_data!(self, left, left_data, "with_two (left)");
            result
        }
    }

    /// Removes all values and resets the ID counter, used between executor runs.
    pub fn clear(&mut self) {
        // When dec-ref-check is enabled, mark all contained Values as Dereferenced
        // before clearing to prevent Drop panics. We use py_dec_ref_ids for this
        // since it handles the marking (we ignore the collected IDs since we're
        // clearing everything anyway).
        #[cfg(feature = "dec-ref-check")]
        {
            let mut dummy_stack = Vec::new();
            for value in self.entries.iter_mut().flatten() {
                if let Some(data) = &mut value.data {
                    data.py_dec_ref_ids(&mut dummy_stack);
                }
            }
        }
        self.entries.clear();
        self.free_list.clear();
    }

    /// Returns the reference count for the heap entry at the given ID.
    ///
    /// This is primarily used for testing reference counting behavior.
    ///
    /// # Panics
    /// Panics if the value ID is invalid or the value has already been freed.
    #[must_use]
    pub fn get_refcount(&self, id: HeapId) -> usize {
        self.entries
            .get(id.index())
            .expect("Heap::get_refcount: slot missing")
            .as_ref()
            .expect("Heap::get_refcount: object already freed")
            .refcount
    }

    /// Returns the number of live (non-freed) values on the heap.
    ///
    /// This is primarily used for testing to verify that all heap entries
    /// are accounted for in reference count tests.
    #[must_use]
    pub fn entry_count(&self) -> usize {
        self.entries.iter().filter(|o| o.is_some()).count()
    }

    /// Gets the value inside a cell, cloning it with proper refcount handling.
    ///
    /// Uses `clone_with_heap` to properly handle all value types including closures,
    /// which need their captured cell refcounts incremented.
    ///
    /// # Panics
    /// Panics if the ID is invalid, the value has been freed, or the entry is not a Cell.
    pub fn get_cell_value(&mut self, id: HeapId) -> Value {
        // Take the data out to avoid borrow conflicts when cloning
        let data = take_data!(self, id, "get_cell_value");

        let result = match &data {
            HeapData::Cell(v) => v.clone_with_heap(self),
            _ => panic!("Heap::get_cell_value: entry is not a Cell"),
        };

        // Restore data before returning
        restore_data!(self, id, data, "get_cell_value");

        result
    }

    /// Sets the value inside a cell, properly dropping the old value.
    ///
    /// # Panics
    /// Panics if the ID is invalid, the value has been freed, or the entry is not a Cell.
    pub fn set_cell_value(&mut self, id: HeapId, value: Value) {
        // Take the data out to avoid borrow conflicts
        let mut data = take_data!(self, id, "set_cell_value");

        match &mut data {
            HeapData::Cell(old_value) => {
                // Swap in the new value
                let old = std::mem::replace(old_value, value);
                // Restore data first, then drop old value
                restore_data!(self, id, data, "set_cell_value");
                old.drop_with_heap(self);
            }
            _ => panic!("Heap::set_cell_value: entry is not a Cell"),
        }
    }

    /// Returns a reference to the value inside a cell without cloning.
    ///
    /// Useful when you only need to read the cell's value temporarily.
    ///
    /// # Panics
    /// Panics if the ID is invalid, the value has been freed, or the entry is not a Cell.
    pub fn get_cell_value_ref(&self, id: HeapId) -> &Value {
        let data = self.get(id);
        match data {
            HeapData::Cell(v) => v,
            _ => panic!("Heap::get_cell_value_ref: entry is not a Cell"),
        }
    }

    /// Helper for List in-place add: extends the destination vec with items from a heap list.
    ///
    /// This method exists to work around borrow checker limitations when List::py_iadd
    /// needs to read from one heap entry while extending another. By keeping both
    /// the read and the refcount increments within Heap's impl block, we can use the
    /// take/restore pattern to avoid the lifetime propagation issues.
    ///
    /// Returns `true` if successful, `false` if the source ID is not a List.
    pub fn iadd_extend_list(&mut self, source_id: HeapId, dest: &mut Vec<Value>) -> bool {
        // Take the source data temporarily
        let source_data = take_data!(self, source_id, "iadd_extend_list");

        let success = if let HeapData::List(list) = &source_data {
            // Copy items and track which refs need incrementing
            let items: Vec<Value> = list.as_vec().iter().map(Value::copy_for_extend).collect();
            let ref_ids: Vec<HeapId> = items
                .iter()
                .filter_map(|obj| if let Value::Ref(id) = obj { Some(*id) } else { None })
                .collect();

            // Restore source data before mutating heap (inc_ref needs it)
            restore_data!(self, source_id, source_data, "iadd_extend_list");

            // Now increment refcounts
            for id in ref_ids {
                self.inc_ref(id);
            }

            // Extend destination
            dest.extend(items);
            true
        } else {
            // Not a list, restore and return false
            restore_data!(self, source_id, source_data, "iadd_extend_list");
            false
        };

        success
    }

    /// Multiplies (repeats) a sequence by an integer count.
    ///
    /// This method handles sequence repetition for Python's `*` operator when applied
    /// to sequences (str, bytes, list, tuple). It creates a new heap-allocated sequence
    /// with the elements repeated `count` times.
    ///
    /// # Arguments
    /// * `id` - HeapId of the sequence to repeat
    /// * `count` - Number of times to repeat (0 returns empty sequence)
    ///
    /// # Returns
    /// * `Ok(Some(Value))` - The new repeated sequence
    /// * `Ok(None)` - If the heap entry is not a sequence type
    /// * `Err` - If allocation fails due to resource limits
    pub fn mult_sequence(&mut self, id: HeapId, count: usize) -> RunResult<Option<Value>> {
        // Take the data out to avoid borrow conflicts
        let data = take_data!(self, id, "mult_sequence");

        let result = match &data {
            HeapData::Str(s) => {
                let repeated = s.as_str().repeat(count);
                restore_data!(self, id, data, "mult_sequence");
                Ok(Some(Value::Ref(self.allocate(HeapData::Str(repeated.into()))?)))
            }
            HeapData::Bytes(b) => {
                let repeated = b.as_slice().repeat(count);
                restore_data!(self, id, data, "mult_sequence");
                Ok(Some(Value::Ref(self.allocate(HeapData::Bytes(repeated.into()))?)))
            }
            HeapData::List(list) => {
                if count == 0 {
                    restore_data!(self, id, data, "mult_sequence");
                    Ok(Some(Value::Ref(self.allocate(HeapData::List(List::new(Vec::new())))?)))
                } else {
                    // Copy items and track which refs need incrementing
                    let items: Vec<Value> = list.as_vec().iter().map(Value::copy_for_extend).collect();
                    let ref_ids: Vec<HeapId> = items
                        .iter()
                        .filter_map(|v| if let Value::Ref(id) = v { Some(*id) } else { None })
                        .collect();
                    let original_len = items.len();

                    // Restore data before heap operations
                    restore_data!(self, id, data, "mult_sequence");

                    // Now increment refcounts for each copy we'll make
                    // We need (count) copies of each ref
                    for ref_id in &ref_ids {
                        for _ in 0..count {
                            self.inc_ref(*ref_id);
                        }
                    }

                    // Build the repeated list with overflow check
                    let capacity = original_len
                        .checked_mul(count)
                        .ok_or_else(ExcType::overflow_repeat_count)?;
                    let mut result = Vec::with_capacity(capacity);
                    for _ in 0..count {
                        for item in &items {
                            result.push(item.copy_for_extend());
                        }
                    }

                    // Manually forget the items vec to avoid Drop panic
                    // The values have been copied to result with proper refcounts
                    std::mem::forget(items);

                    Ok(Some(Value::Ref(self.allocate(HeapData::List(List::new(result)))?)))
                }
            }
            HeapData::Tuple(tuple) => {
                if count == 0 {
                    restore_data!(self, id, data, "mult_sequence");
                    Ok(Some(Value::Ref(
                        self.allocate(HeapData::Tuple(Tuple::new(Vec::new())))?,
                    )))
                } else {
                    // Copy items and track which refs need incrementing
                    let items: Vec<Value> = tuple.as_vec().iter().map(Value::copy_for_extend).collect();
                    let ref_ids: Vec<HeapId> = items
                        .iter()
                        .filter_map(|v| if let Value::Ref(id) = v { Some(*id) } else { None })
                        .collect();
                    let original_len = items.len();

                    // Restore data before heap operations
                    restore_data!(self, id, data, "mult_sequence");

                    // Now increment refcounts for each copy we'll make
                    // We need (count) copies of each ref
                    for ref_id in &ref_ids {
                        for _ in 0..count {
                            self.inc_ref(*ref_id);
                        }
                    }

                    // Build the repeated tuple with overflow check
                    let capacity = original_len
                        .checked_mul(count)
                        .ok_or_else(ExcType::overflow_repeat_count)?;
                    let mut result = Vec::with_capacity(capacity);
                    for _ in 0..count {
                        for item in &items {
                            result.push(item.copy_for_extend());
                        }
                    }

                    // Manually forget the items vec to avoid Drop panic
                    std::mem::forget(items);

                    Ok(Some(Value::Ref(self.allocate(HeapData::Tuple(Tuple::new(result)))?)))
                }
            }
            _ => {
                // Dicts, Cells, Callables, Functions and Closures don't support multiplication
                restore_data!(self, id, data, "mult_sequence");
                Ok(None)
            }
        };

        result
    }

    /// Runs mark-sweep garbage collection to free unreachable cycles.
    ///
    /// This method takes a closure that provides an iterator of root HeapIds
    /// (typically from Namespaces). It marks all reachable objects starting
    /// from roots, then sweeps (frees) any unreachable objects.
    ///
    /// This is necessary because reference counting alone cannot free cycles
    /// where objects reference each other but are unreachable from the program.
    ///
    /// # Arguments
    /// * `get_roots` - Closure returning an iterator of HeapIds that are roots
    pub fn collect_garbage<I, F>(&mut self, get_roots: F)
    where
        I: Iterator<Item = HeapId>,
        F: FnOnce() -> I,
    {
        // Mark phase: collect all reachable IDs using BFS
        let mut reachable: HashSet<HeapId> = HashSet::new();
        let mut work_list: Vec<HeapId> = get_roots().collect();

        while let Some(id) = work_list.pop() {
            if !reachable.insert(id) {
                continue; // Already visited
            }

            // Add children to work list
            if let Some(Some(entry)) = self.entries.get(id.index()) {
                if let Some(ref data) = entry.data {
                    self.collect_child_ids(data, &mut work_list);
                }
            }
        }

        // Sweep phase: free unreachable values
        for (id, value) in self.entries.iter_mut().enumerate() {
            let heap_id = HeapId(id);
            if reachable.contains(&heap_id) {
                continue;
            }

            // This entry is unreachable - free it
            if let Some(value) = value.take() {
                // Notify tracker of freed memory
                if let Some(ref data) = value.data {
                    self.tracker.on_free(|| data.py_estimate_size());
                }

                self.free_list.push(heap_id);

                // Mark Values as Dereferenced when dec-ref-check is enabled
                #[cfg(feature = "dec-ref-check")]
                if let Some(mut data) = value.data {
                    data.py_dec_ref_ids(&mut Vec::new());
                }
            }
        }

        // Notify tracker that GC is complete
        self.tracker.on_gc_complete();
    }

    /// Collects child HeapIds from a HeapData value for GC traversal.
    fn collect_child_ids(&self, data: &HeapData, work_list: &mut Vec<HeapId>) {
        match data {
            HeapData::Str(_) | HeapData::Bytes(_) => {}
            HeapData::List(list) => {
                for value in list.as_vec() {
                    if let Value::Ref(id) = value {
                        work_list.push(*id);
                    }
                }
            }
            HeapData::Tuple(tuple) => {
                for value in tuple.as_vec() {
                    if let Value::Ref(id) = value {
                        work_list.push(*id);
                    }
                }
            }
            HeapData::Dict(dict) => {
                // Iterate through all entries to collect heap references
                for (k, v) in dict {
                    if let Value::Ref(id) = k {
                        work_list.push(*id);
                    }
                    if let Value::Ref(id) = v {
                        work_list.push(*id);
                    }
                }
            }
            HeapData::Set(set) => {
                for value in set.storage().iter() {
                    if let Value::Ref(id) = value {
                        work_list.push(*id);
                    }
                }
            }
            HeapData::FrozenSet(frozenset) => {
                for value in frozenset.storage().iter() {
                    if let Value::Ref(id) = value {
                        work_list.push(*id);
                    }
                }
            }
            HeapData::Closure(_, cells, defaults) => {
                // Add captured cells to work list
                for cell_id in cells {
                    work_list.push(*cell_id);
                }
                // Add default values that are heap references
                for default in defaults {
                    if let Value::Ref(id) = default {
                        work_list.push(*id);
                    }
                }
            }
            HeapData::FunctionDefaults(_, defaults) => {
                // Add default values that are heap references
                for default in defaults {
                    if let Value::Ref(id) = default {
                        work_list.push(*id);
                    }
                }
            }
            HeapData::Cell(value) => {
                // Cell can contain a reference to another heap value
                if let Value::Ref(id) = value {
                    work_list.push(*id);
                }
            }
        }
    }
}

/// Drop implementation for Heap that marks all contained Objects as Dereferenced
/// before dropping to prevent panics when the `dec-ref-check` feature is enabled.
#[cfg(feature = "dec-ref-check")]
impl<T: ResourceTracker> Drop for Heap<T> {
    fn drop(&mut self) {
        // Mark all contained Objects as Dereferenced before dropping.
        // We use py_dec_ref_ids for this since it handles the marking
        // (we ignore the collected IDs since we're dropping everything anyway).
        let mut dummy_stack = Vec::new();
        for value in self.entries.iter_mut().flatten() {
            if let Some(data) = &mut value.data {
                data.py_dec_ref_ids(&mut dummy_stack);
            }
        }
    }
}
